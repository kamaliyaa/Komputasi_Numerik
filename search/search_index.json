{"config":{"lang":["en"],"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Selamat Datang Di halaman Penambangan Data \u00b6 Nama : Kamaliya NIM : 180411100030 Kelas : Penambangan Data 5D Dosen Pengampu : Mula'ab,S.SI.,M.kom Jurusan : Teknik Informatika Alamat : Jln.Sukun VI ,Perumnas Kamal \u00b6","title":"index"},{"location":"#selamat-datang-di-halaman-penambangan-data","text":"Nama : Kamaliya NIM : 180411100030 Kelas : Penambangan Data 5D Dosen Pengampu : Mula'ab,S.SI.,M.kom Jurusan : Teknik Informatika Alamat : Jln.Sukun VI ,Perumnas Kamal","title":"Selamat Datang Di halaman Penambangan Data"},{"location":"Mengukur Jarak/","text":"Mengukur Jarak Data \u00b6 Mengukur Jarak Numerik \u00b6 Salah satu tantangan dalam era ini dengan datatabase yang memiliki banyak tipe data. Mengukur jarak adalah komponen utama dalam algoritma clustering berbasis jarak. Alogritma seperit Algoritma Partisioning misal K-Mean, K-medoidm dan fuzzy c-mean dan rough clustering bergantung pada jarak untuk melakukan pengelompokkan Sebelum menjelaskan tentang beberapa macam ukuran jarak, kita mendefinisikan terlebih dahulu yaiut v1,v2 menyatakandua vektor yang menyatakan v1=x1,x2,...,xn,v2=y1,y2,...,yn, dimana xi,yi disebut attribut. Ada beberapa ukuran similaritas datau ukuran jarak, diantaranya Minkowski Distance \u00b6 kelompok Minkowski diantaranya adalah Euclidean distance dan Manhattan distance, yang menjadi kasus khusus dari Minkowski Distance.Minkowski Distance dinyatakan dengan $$ d _ { \\operatorname { min } } = ( \\ sum _ { i = 1 } ^ { n } | x _ { i } - y _ { i } | ^ { m } ) ^ { \\frac { 1 } { m } } , m \\geq 1 $$ Uuclidean Distance \u00b6 Jarak yang paling terkenal yang digunakan untuk data numerik adalah jarak Euclidean. Ini adalah kasus khusus dari jarak Minkowski ketika m = 2. Jarak Euclidean berkinerja baik ketika digunakan untuk kumpulan data cluster kompak atau terisolasi . Meskipun jarak Euclidean sangat umum dalam pengelompokan, ia memiliki kelemahan: jika dua vektor data tidak memiliki nilai atribut yang sama, kemungkin memiliki jarak yang lebih kecil daripada pasangan vektor data lainnya yang mengandung nilai atribut yang sama. Masalah lain dengan jarak Euclidean sebagai fitur skala terbesar akan mendominasi yang lain. Normalisasi fitur kontinu adalah solusi untuk mengatasi kelemahan ini Average Distance \u00b6 Berkenaan dengan kekurangan dari Jarak Euclidian Distance diatas, rata rata jarak adala versi modikfikasid ari jarak Euclidian untuk memperbaiki hasil. Untuk dua titik x,yx,y dalam ruang dimensi nn, rata-rata jarak didefinisikan dengan Average Distance $$ d _ { a v e } = \\left ( \\frac { 1 } { n } \\sum _ { i = 1 } ^ { n } ( x _ { i } - y _ { i } ) ^ { 2 } \\right) ^ { \\frac { 1 } { 2 } } $$ Manhattan distance \u00b6 Manhattan distance adalah kasus khsusu dari jarak Minkowski distance pada m = 1. Seperti Minkowski Distance, Manhattan distance sensitif terhadap outlier. BIla ukuran ini digunakan dalam algoritma clustering , bentuk cluster adalah hyper-rectangular. Ukuran ini didefinisikan dengan $$ d _ { \\operatorname { man } } = \\sum _ { i = 1 } ^ { n } \\left| x _ { i } - y _ { i } \\right| $$ \u00b6 Weighted euclidean distance \u00b6 Jika berdasarkan tingkatan penting dari masing masing atribut ditentukan, maka Weighted Euclidean distance adalah modifikisasi lain dari jarak Euclidean distance yang dapat digunakan. Ukuran ini dirumuskan dengan $$ d _ { w e } = \\left ( \\sum _ { i = 1 } ^ { n } w _ { i } ( x _ { i } - y _ { i } \\right) ^ { 2 } ) ^ { \\frac { 1 } { 2 } } $$ dimana wi adalah bobot yang diberikan pada atribut ke i Chord distance \u00b6 Chord distance adalah satu ukuran dengan jarak modifikasi Euclidean distance agar mengatasi kekurangan dari Euclidean distance. dipecahkan juga dengan cara skala pengukuran yang baik. Jarak ini dapat juga dihitung dari data yang tidak dinormalisasi . Chord distance didefinisikan dengan $$ d _ { \\text {chord} } = \\left ( 2 - 2 \\frac { \\sum _ { i = 1 } ^ { n } x _ { i } y _ { i } } { | x | _ { 2 } | y | _ { 2 } } \\right) ^ { \\frac { 1 } { 2 } } $$ dimana \u2225x\u22252\u2016x\u20162 adalah L2-norm\u2225x\u22252= $$ L^{2} \\text {-norm} | x | {2} = \\sqrt { \\sum { i = 1 }^{ n }x_{i}^{2}} $$ MathJax.Hub.Config({ tex2jax: {inlineMath:[['$$','$$']]} }); \u200b \u200b","title":"mengukur jarak"},{"location":"Mengukur Jarak/#mengukur-jarak-data","text":"","title":"Mengukur Jarak Data"},{"location":"Mengukur Jarak/#mengukur-jarak-numerik","text":"Salah satu tantangan dalam era ini dengan datatabase yang memiliki banyak tipe data. Mengukur jarak adalah komponen utama dalam algoritma clustering berbasis jarak. Alogritma seperit Algoritma Partisioning misal K-Mean, K-medoidm dan fuzzy c-mean dan rough clustering bergantung pada jarak untuk melakukan pengelompokkan Sebelum menjelaskan tentang beberapa macam ukuran jarak, kita mendefinisikan terlebih dahulu yaiut v1,v2 menyatakandua vektor yang menyatakan v1=x1,x2,...,xn,v2=y1,y2,...,yn, dimana xi,yi disebut attribut. Ada beberapa ukuran similaritas datau ukuran jarak, diantaranya","title":"Mengukur Jarak Numerik"},{"location":"Mengukur Jarak/#minkowski-distance","text":"kelompok Minkowski diantaranya adalah Euclidean distance dan Manhattan distance, yang menjadi kasus khusus dari Minkowski Distance.Minkowski Distance dinyatakan dengan $$ d _ { \\operatorname { min } } = ( \\ sum _ { i = 1 } ^ { n } | x _ { i } - y _ { i } | ^ { m } ) ^ { \\frac { 1 } { m } } , m \\geq 1 $$","title":"Minkowski Distance"},{"location":"Mengukur Jarak/#uuclidean-distance","text":"Jarak yang paling terkenal yang digunakan untuk data numerik adalah jarak Euclidean. Ini adalah kasus khusus dari jarak Minkowski ketika m = 2. Jarak Euclidean berkinerja baik ketika digunakan untuk kumpulan data cluster kompak atau terisolasi . Meskipun jarak Euclidean sangat umum dalam pengelompokan, ia memiliki kelemahan: jika dua vektor data tidak memiliki nilai atribut yang sama, kemungkin memiliki jarak yang lebih kecil daripada pasangan vektor data lainnya yang mengandung nilai atribut yang sama. Masalah lain dengan jarak Euclidean sebagai fitur skala terbesar akan mendominasi yang lain. Normalisasi fitur kontinu adalah solusi untuk mengatasi kelemahan ini","title":"Uuclidean Distance"},{"location":"Mengukur Jarak/#average-distance","text":"Berkenaan dengan kekurangan dari Jarak Euclidian Distance diatas, rata rata jarak adala versi modikfikasid ari jarak Euclidian untuk memperbaiki hasil. Untuk dua titik x,yx,y dalam ruang dimensi nn, rata-rata jarak didefinisikan dengan Average Distance $$ d _ { a v e } = \\left ( \\frac { 1 } { n } \\sum _ { i = 1 } ^ { n } ( x _ { i } - y _ { i } ) ^ { 2 } \\right) ^ { \\frac { 1 } { 2 } } $$","title":"Average Distance"},{"location":"Mengukur Jarak/#manhattan-distance","text":"Manhattan distance adalah kasus khsusu dari jarak Minkowski distance pada m = 1. Seperti Minkowski Distance, Manhattan distance sensitif terhadap outlier. BIla ukuran ini digunakan dalam algoritma clustering , bentuk cluster adalah hyper-rectangular. Ukuran ini didefinisikan dengan $$ d _ { \\operatorname { man } } = \\sum _ { i = 1 } ^ { n } \\left| x _ { i } - y _ { i } \\right| $$","title":"Manhattan distance"},{"location":"Mengukur Jarak/#weighted-euclidean-distance","text":"Jika berdasarkan tingkatan penting dari masing masing atribut ditentukan, maka Weighted Euclidean distance adalah modifikisasi lain dari jarak Euclidean distance yang dapat digunakan. Ukuran ini dirumuskan dengan $$ d _ { w e } = \\left ( \\sum _ { i = 1 } ^ { n } w _ { i } ( x _ { i } - y _ { i } \\right) ^ { 2 } ) ^ { \\frac { 1 } { 2 } } $$ dimana wi adalah bobot yang diberikan pada atribut ke i","title":"Weighted euclidean distance"},{"location":"Mengukur Jarak/#chord-distance","text":"Chord distance adalah satu ukuran dengan jarak modifikasi Euclidean distance agar mengatasi kekurangan dari Euclidean distance. dipecahkan juga dengan cara skala pengukuran yang baik. Jarak ini dapat juga dihitung dari data yang tidak dinormalisasi . Chord distance didefinisikan dengan $$ d _ { \\text {chord} } = \\left ( 2 - 2 \\frac { \\sum _ { i = 1 } ^ { n } x _ { i } y _ { i } } { | x | _ { 2 } | y | _ { 2 } } \\right) ^ { \\frac { 1 } { 2 } } $$ dimana \u2225x\u22252\u2016x\u20162 adalah L2-norm\u2225x\u22252= $$ L^{2} \\text {-norm} | x | {2} = \\sqrt { \\sum { i = 1 }^{ n }x_{i}^{2}} $$ MathJax.Hub.Config({ tex2jax: {inlineMath:[['$$','$$']]} }); \u200b \u200b","title":"Chord distance"},{"location":"Statistik Dekriptif/","text":"Statistik Deskriptif \u00b6 Pengertian \u00b6 Pengertian statistik deskriptif metode pengumpulan sebuah data data yang akan menghasilkan informasi yang berguna Tipe statistik deskriptif \u00b6 Mean(Rata-rata) Mean atau rata rata adalah sebuah nilai yang jumlah dari semua angka atau data dapat di bagi dari banyak data itu .misal memiliki N data dapat di hitung dengan rumus mean sebagai berikut : $$ \\begin{align} \\bar{X} = \\frac{1}{n} \\sum_{i=1}^n X_i={a_1+a_2+a_3+a_4+........+a_n \\over n} \\end{align} $$ keterangan: x=rata-rata a=nilai ke N n=banyak nilai atau data Median median merupakan nilai tengah (pusat data) dalam suatu data median biasanya bisa disebut Me .menghitung median mempunyai 2 metode yaitu ketika N atau jumlah data ganjil atau genap. saat data ganjil dan data genap perhitingan nya berbeda.berikut rumus median yang dapat di gunakan : $$ Me=Q_2 =\\left( \\begin{matrix} n+1 \\over 2 \\end{matrix} \\right), jika\\quad n\\quad ganjil $$ $$ Me=Q_2 =\\left( \\begin{matrix} {xn \\over 2 } {xn+1\\over 2} \\over 2 \\end{matrix} \\right), jika\\quad n\\quad genap $$ keterangan: me =median atau nilai tengah n=banyak data Modus \u00b6 Modus adalah nilai yang sering muncul dalam himpunan data dan jika hasil dengan jumlah nilai tertinggi maka itu merupakan modus dari himpunan angka. brikut ini rumus mencari modus dalam himpunan data : $$ M_o = Tb + p{b_1 \\over b_1 + b_2} $$ ket; mo=nilai modus tb= tepi bawah b1=selisih frekuensi antara nilai mudus dengan elemen sebelumnya b2=selisih frekuensi antara nilai mudus dengan elemen sesudahnya p= panjang interval Varian \u00b6 Varian adalah penyebaran nilai dalam suatu data dari rata rata .berikut ini rumus yang dapat di gunakan : $$ \\sigma^2 = {\\sum \\limits_{i=1}^{n} (x_i - \\bar x)^2 \\over n} $$ Keterangan: x=rata rata Xi=rata rata dari semua titik data n= banyak dari anggota data \u200b Standart Deviasi \u00b6 Standar deviasi adalah ukuran kumpulan data relatif terhadap rata-rata atau akar kuadrat positif dari varian. standar deviasi di hitung dengan cara mengakar kuadrat nilai dari varians. dengan menggunakan rumus standar varian berikut : $$ t {{\\sum \\limits_{i=1}^{n} (x_i - \\bar x)^2 \\over n}} $$ Skewness \u00b6 adalah ketidaksimetrisan atau kemiringan pada suatu kurva yang tampak condong ke kiri atau ke kanan. Skewness bisa dihitung menggunakan rumus sebagai berikut: $$ {\\sum \\limits{i=1}^n (x_i - \\bar x)^i \\over (n- 1) \\sigma^3} $$ Quartile \u00b6 Quartile adalah irisan nilai dari hasil pembagian data menjadi empat bagian yang sama besar.quartil terbagi menjadi tiga yaitu quartil pertama,kedua,dan ketiga. $$ Q_1 = (n + 1) {1\\over 4} $$ $$ Q_2 = (n + 1) {1\\over 2} $$ $$ Q_3 = (n + 1) {3\\over 4} $$ Penerapan Statistik Deskriptif Menggunakan Python \u00b6 Alat dan Bahan \u00b6 Pada penerapan ini saya menggunakan 500 data random yang disimpan dalam bentuk .csv dan untuk mempermudah dalam penerapan tersebut, perlu disiapkan library python yang dapat didownload secara gratis. dalam kasus ini, library python yang digunakan adalah sebagai berikut: pertama : pandas, digunakan untuk data manajemen dan data analysis. kedua : scipy, merupakan library berisi kumpulan algoritma dan fungsi matematika. lebih jelasnya bisa di lihat di bawah ini : \u00b6 import pandas as pd from scipy import stats df = pd . read_csv ( \"liya.csv\" , sep = ',' ) data = { \"Stats\" :[ 'Min' , 'Max' , 'Mean' , 'Standard Deviasi' , 'Variasi' , 'Skewnes' , 'Quantile 1' , 'Quantile 2' , 'Quantile 3' , 'Median' , 'Modus' ]} for i in df . columns : data [ i ] = [ df [ i ] . min (), df [ i ] . max (), df [ i ] . mean (), round ( df [ i ] . std (), 2 ), round ( df [ i ] . var (), 2 ), round ( df [ i ] . skew (), 2 ), df [ i ] . quantile ( 0.25 ), df [ i ] . quantile ( 0.5 ), df [ i ] . quantile ( 0.75 ), df [ i ] . median (), stats . mode ( df [ i ]) . mode [ 0 ]] tes = pd . DataFrame ( data ) tes . style . hide_index () Hasil Running \u00b6 stats Laki-Laki Perempuan Tinggi Badan Berat Badan Min 20 20 150 50 Max 40 30 180 75 Mean 30.01 25.06 165.284 62.61 Standard Deviasi 6.21 3.08 8.83 7.6 Variasi 38.57 9.46 77.92 57.81 Skewnes -0.07 0.01 -0.11 -0.02 Quantile 1 25 23 157 57 Quantile 2 30 25 166 62 Quantile 3 36 28 173 70 Median 30 25 166 62 Modus 37 24 167 57 MathJax.Hub.Config({ tex2jax: {inlineMath:[['$$','$$']]} });","title":"statistik deskriptif"},{"location":"Statistik Dekriptif/#statistik-deskriptif","text":"","title":"Statistik Deskriptif"},{"location":"Statistik Dekriptif/#pengertian","text":"Pengertian statistik deskriptif metode pengumpulan sebuah data data yang akan menghasilkan informasi yang berguna","title":"Pengertian"},{"location":"Statistik Dekriptif/#tipe-statistik-deskriptif","text":"Mean(Rata-rata) Mean atau rata rata adalah sebuah nilai yang jumlah dari semua angka atau data dapat di bagi dari banyak data itu .misal memiliki N data dapat di hitung dengan rumus mean sebagai berikut : $$ \\begin{align} \\bar{X} = \\frac{1}{n} \\sum_{i=1}^n X_i={a_1+a_2+a_3+a_4+........+a_n \\over n} \\end{align} $$ keterangan: x=rata-rata a=nilai ke N n=banyak nilai atau data Median median merupakan nilai tengah (pusat data) dalam suatu data median biasanya bisa disebut Me .menghitung median mempunyai 2 metode yaitu ketika N atau jumlah data ganjil atau genap. saat data ganjil dan data genap perhitingan nya berbeda.berikut rumus median yang dapat di gunakan : $$ Me=Q_2 =\\left( \\begin{matrix} n+1 \\over 2 \\end{matrix} \\right), jika\\quad n\\quad ganjil $$ $$ Me=Q_2 =\\left( \\begin{matrix} {xn \\over 2 } {xn+1\\over 2} \\over 2 \\end{matrix} \\right), jika\\quad n\\quad genap $$ keterangan: me =median atau nilai tengah n=banyak data","title":"Tipe statistik deskriptif"},{"location":"Statistik Dekriptif/#modus","text":"Modus adalah nilai yang sering muncul dalam himpunan data dan jika hasil dengan jumlah nilai tertinggi maka itu merupakan modus dari himpunan angka. brikut ini rumus mencari modus dalam himpunan data : $$ M_o = Tb + p{b_1 \\over b_1 + b_2} $$ ket; mo=nilai modus tb= tepi bawah b1=selisih frekuensi antara nilai mudus dengan elemen sebelumnya b2=selisih frekuensi antara nilai mudus dengan elemen sesudahnya p= panjang interval","title":"Modus"},{"location":"Statistik Dekriptif/#varian","text":"Varian adalah penyebaran nilai dalam suatu data dari rata rata .berikut ini rumus yang dapat di gunakan : $$ \\sigma^2 = {\\sum \\limits_{i=1}^{n} (x_i - \\bar x)^2 \\over n} $$ Keterangan: x=rata rata Xi=rata rata dari semua titik data n= banyak dari anggota data \u200b","title":"Varian"},{"location":"Statistik Dekriptif/#standart-deviasi","text":"Standar deviasi adalah ukuran kumpulan data relatif terhadap rata-rata atau akar kuadrat positif dari varian. standar deviasi di hitung dengan cara mengakar kuadrat nilai dari varians. dengan menggunakan rumus standar varian berikut : $$ t {{\\sum \\limits_{i=1}^{n} (x_i - \\bar x)^2 \\over n}} $$","title":"Standart Deviasi"},{"location":"Statistik Dekriptif/#skewness","text":"adalah ketidaksimetrisan atau kemiringan pada suatu kurva yang tampak condong ke kiri atau ke kanan. Skewness bisa dihitung menggunakan rumus sebagai berikut: $$ {\\sum \\limits{i=1}^n (x_i - \\bar x)^i \\over (n- 1) \\sigma^3} $$","title":"Skewness"},{"location":"Statistik Dekriptif/#quartile","text":"Quartile adalah irisan nilai dari hasil pembagian data menjadi empat bagian yang sama besar.quartil terbagi menjadi tiga yaitu quartil pertama,kedua,dan ketiga. $$ Q_1 = (n + 1) {1\\over 4} $$ $$ Q_2 = (n + 1) {1\\over 2} $$ $$ Q_3 = (n + 1) {3\\over 4} $$","title":"Quartile"},{"location":"Statistik Dekriptif/#penerapan-statistik-deskriptif-menggunakan-python","text":"","title":"Penerapan Statistik Deskriptif Menggunakan Python"},{"location":"Statistik Dekriptif/#alat-dan-bahan","text":"Pada penerapan ini saya menggunakan 500 data random yang disimpan dalam bentuk .csv dan untuk mempermudah dalam penerapan tersebut, perlu disiapkan library python yang dapat didownload secara gratis. dalam kasus ini, library python yang digunakan adalah sebagai berikut: pertama : pandas, digunakan untuk data manajemen dan data analysis. kedua : scipy, merupakan library berisi kumpulan algoritma dan fungsi matematika. lebih jelasnya bisa di lihat di bawah ini :","title":"Alat dan Bahan"},{"location":"Statistik Dekriptif/#hasil-running","text":"stats Laki-Laki Perempuan Tinggi Badan Berat Badan Min 20 20 150 50 Max 40 30 180 75 Mean 30.01 25.06 165.284 62.61 Standard Deviasi 6.21 3.08 8.83 7.6 Variasi 38.57 9.46 77.92 57.81 Skewnes -0.07 0.01 -0.11 -0.02 Quantile 1 25 23 157 57 Quantile 2 30 25 166 62 Quantile 3 36 28 173 70 Median 30 25 166 62 Modus 37 24 167 57 MathJax.Hub.Config({ tex2jax: {inlineMath:[['$$','$$']]} });","title":"Hasil Running"},{"location":"missing value/","text":"Memperlakukan Missing Value Dengan Metode Algoritma K-Nearest Neighbor (KNN) \u00b6 Missing Value \u00b6 Missing value adalah informasi yang tidak tersedia untuk sebuah objek (kasus). Missing value terjadi karena informasi untuk sesuatu tentang objek tidak diberikan, sulit dicari, atau memang informasi tersebut tidak ada. Missing value pada dasarnya tidak bermasalah bagi keseluruhan data, apalagi jika jumlahnya hanya sedikit, misal hanya 1 % dari seluruh data. Namun jika persentase data yang hilang tersebut cukup besar, maka perlu dilakukan pengujian apakah data yang meng kita dapat menggunakan metode Algoritma K-Nearest Neighbor (KKN). K-Nearest Neighbor (KNN) \u00b6 Salah satu usaha untuk memperlakukan missing data yaitu dengan menggunakan Algoritma K-Nearest Neighbor (KNN). Lalu apa yang dimaksud dengan KNN? Algoritma K-Nearest Neighbor (K-NN) adalah sebuah metode klasifikasi terhadap sekumpulan data berdasarkan pembelajaran data yang sudah terklasifikasikan sebelumya. Termasuk dalam supervised learning , dimana hasil query instance yang baru diklasifikasikan berdasarkan mayoritas kedekatan jarak dari kategori yang ada dalam K-NN . Algroritma pada K-Nearest Neighbor (KNN) \u00b6 Langkah utama dalam metode KNN yaitu dengan menghitung nilai k. Nilai k yang dimaksud yaitu jarak tetangga terdekat antar dataset. Kemudian, hasil perhitungan nilai k tersebut menjadi nilai estimator yang digunakan untuk mengisi pada data yang hilang tersebut. Perhitungan untuk mencari nilai k yaitu tergantung dengan jenis data. Apabila data yang disajikan berupa data kontinu, maka menggunakan rata-rata dari tetangga terdekat. Dan apabila data yang disajikan berupa data kualitatif, maka diambil dari nilai yang sering keluar pada objek. Dapat dimisalkan bahwa D merupakan suatu objek yang memiliki kasus missing data. Dengan Dc merupakan subdata yang lengkap, sedangkan Dm merupakan sub data yang memiliki kerumpangan (mengandung atribut yang hilang). Maka, tahapan langkah algoritma pada KKN sebagai berikut : Menentukan parameter k (jumlah tetangga paling dekat). Menghitung kuadrat jarak eucliden objek terhadap data training yang diberikan. Mengurutkan hasil no 2 secara ascending (berurutan dari nilai tinggi ke rendah) Mengumpulkan kategori Y (Klasifikasi nearest neighbor berdasarkan nilai k) Dengan menggunakan kategori nearest neighbor yang paling mayoritas maka dapat dipredisikan kategori objek. # importing pandas as pd import pandas as pd # importing numpy as np import numpy as np # dictionary of lists dict = { 'First Score' :[ 100 , 80 , np . nan , 65 ], 'Second Score' : [ 80 , 55 , 76 , np . nan ], 'Third Score' :[ np . nan , 60 , 90 , 87 ]} # creating a dataframe from dictionary df = pd . DataFrame ( dict ) # filling missing value using fillna() df . fillna ( 0 ) First Score Second Score Third Score 0 100.0 80.0 0.0 1 80.0 55.0 60.0 2 0.0 76.0 90.0 3 65.0 0.0 87.0 MathJax.Hub.Config({ tex2jax: {inlineMath:[['$$','$$']]} }); \u200b","title":"missing value"},{"location":"missing value/#memperlakukan-missing-value-dengan-metode-algoritma-k-nearest-neighbor-knn","text":"","title":"Memperlakukan Missing Value Dengan Metode  Algoritma K-Nearest Neighbor (KNN)"},{"location":"missing value/#missing-value","text":"Missing value adalah informasi yang tidak tersedia untuk sebuah objek (kasus). Missing value terjadi karena informasi untuk sesuatu tentang objek tidak diberikan, sulit dicari, atau memang informasi tersebut tidak ada. Missing value pada dasarnya tidak bermasalah bagi keseluruhan data, apalagi jika jumlahnya hanya sedikit, misal hanya 1 % dari seluruh data. Namun jika persentase data yang hilang tersebut cukup besar, maka perlu dilakukan pengujian apakah data yang meng kita dapat menggunakan metode Algoritma K-Nearest Neighbor (KKN).","title":"Missing Value"},{"location":"missing value/#k-nearest-neighbor-knn","text":"Salah satu usaha untuk memperlakukan missing data yaitu dengan menggunakan Algoritma K-Nearest Neighbor (KNN). Lalu apa yang dimaksud dengan KNN? Algoritma K-Nearest Neighbor (K-NN) adalah sebuah metode klasifikasi terhadap sekumpulan data berdasarkan pembelajaran data yang sudah terklasifikasikan sebelumya. Termasuk dalam supervised learning , dimana hasil query instance yang baru diklasifikasikan berdasarkan mayoritas kedekatan jarak dari kategori yang ada dalam K-NN .","title":"K-Nearest Neighbor (KNN)"},{"location":"missing value/#algroritma-pada-k-nearest-neighbor-knn","text":"Langkah utama dalam metode KNN yaitu dengan menghitung nilai k. Nilai k yang dimaksud yaitu jarak tetangga terdekat antar dataset. Kemudian, hasil perhitungan nilai k tersebut menjadi nilai estimator yang digunakan untuk mengisi pada data yang hilang tersebut. Perhitungan untuk mencari nilai k yaitu tergantung dengan jenis data. Apabila data yang disajikan berupa data kontinu, maka menggunakan rata-rata dari tetangga terdekat. Dan apabila data yang disajikan berupa data kualitatif, maka diambil dari nilai yang sering keluar pada objek. Dapat dimisalkan bahwa D merupakan suatu objek yang memiliki kasus missing data. Dengan Dc merupakan subdata yang lengkap, sedangkan Dm merupakan sub data yang memiliki kerumpangan (mengandung atribut yang hilang). Maka, tahapan langkah algoritma pada KKN sebagai berikut : Menentukan parameter k (jumlah tetangga paling dekat). Menghitung kuadrat jarak eucliden objek terhadap data training yang diberikan. Mengurutkan hasil no 2 secara ascending (berurutan dari nilai tinggi ke rendah) Mengumpulkan kategori Y (Klasifikasi nearest neighbor berdasarkan nilai k) Dengan menggunakan kategori nearest neighbor yang paling mayoritas maka dapat dipredisikan kategori objek. # importing pandas as pd import pandas as pd # importing numpy as np import numpy as np # dictionary of lists dict = { 'First Score' :[ 100 , 80 , np . nan , 65 ], 'Second Score' : [ 80 , 55 , 76 , np . nan ], 'Third Score' :[ np . nan , 60 , 90 , 87 ]} # creating a dataframe from dictionary df = pd . DataFrame ( dict ) # filling missing value using fillna() df . fillna ( 0 ) First Score Second Score Third Score 0 100.0 80.0 0.0 1 80.0 55.0 60.0 2 0.0 76.0 90.0 3 65.0 0.0 87.0 MathJax.Hub.Config({ tex2jax: {inlineMath:[['$$','$$']]} }); \u200b","title":"Algroritma pada K-Nearest Neighbor (KNN)"},{"location":"pohon keputusan/","text":"DECISION TREE \u00b6 Decision tree adalah salah satu metode klasifikasi yang paling populer, karena mudah untuk diinterpretasi oleh manusia. Decision tree merupakan model prediksi menggunakan struktur pohon atau struktur berhirarki. Setiap orang tentu menginginkan sebuah pengambilan keputusan yang tepat dan efisien tak terkecuali sebuah perusahaan. Untuk itu banyak sekali perusahaan yang membutuhkan suatu media seperti Business Intellegence guna membantu dalam pengambilan keputusan yang tepat. Namun, hal tersebut tidak akan berarti tanpa adanya konsep decision tree atau yang biasa disebut pohon keputusan. Konsep dari pohon keputusan adalah mengubah data menjadi decision tree dan aturan-aturan keputusan. Manfaat utama dari penggunaan decision tree adalah kemampuannya untuk mem- break down proses pengambilan keputusan yang kompleks menjadi lebih simple, sehingga pengambil keputusan akan lebih menginterpretasikan solusi dari permasalahan. Nama lain dari decision tree adalah CART ( Classification and Regression Tree ). Dimana metode ini merupakan gabungan dari dua jenis pohon, yaitu classification tree dan juga regression tree . Untuk memudahkan, berikut ilustrasi dari keduanya. Decision tree juga berguna untuk mengeksplorasi data, menemukan hubungan tersembunyi antara sejumlah calon variabel input dengan sebuah variabel target. Decision tree memadukan antara eksplorasi data dan pemodelan, sehingga sangat bagus sebagai langkah awal dalam proses pemodelan bahkan ketika dijadikan sebagai model akhir dari beberapa teknik lain. Entropy \u00b6 Entropi adalah nilai informasi yang menyatakan ukuran ketidakpastian(impurity) dari attribut dari suatu kumpulan obyek data dalam satuan bit. $$ E(T) = \\sum_{i=1}^n {-P_i\\log{P_i}} $$ Dimana : T = ruang sampel data yang digunakan untukdata pelatihan Pi = Probabiliti muncul dalam row Gain \u00b6 Information Gain adalah ukuran efektifitas suatu atribut dlm mengklasifikasikan data, digunakan untuk menentukan urutan atribut dimana attribut yang memiliki nilai Information Gain terbesar yang dipilih. $$ \\operatorname{Gain}(T, X) = \\operatorname{Entropy}(T) - \\sum_{v\\in{T}} \\frac{T_{X,v}}{T} E(T_{X,v}) $$ Dimana : Entropy (T) = nilai entropi total dari atribut keputusan dalam ruang sampel data T x = fitur CARA MEMBUAT DECISION TREE \u00b6 \u200b Ada beberapa cara membuat decision tree disini saya akan membuat dengan cara mengurutkan poperty yang paling penting.sebulum itu kita harus tau rumus rumusnya berikut ini rumus dari entropy dan gain : $$ Entropy(S)={\\sum \\limits_{i=1}^{n} -pi\\quad log_2\\quad pi} $$ keterangan: S=Himpunan kasus n = jumlah partisi S pi= proposi Si terhadap S kemudian hitung nilai gain menggunakan rumus : $$ GAIN(S,A)= entropy(S)-{\\sum \\limits_{i=1}^{n} \\frac{|Si|}{|s|}*entropy(Si)} $$ keterangan: S=himpunan kasus n=jumlah partisi S |si|=proporsi terhadap S |s|=jumlah kasus dalam S untuk mempermudah penghitungan saya menggunakan fungsi pembantu, seperti fungsi banyak_elemen untuk mengecek ada berapa elemen dalam sebuah kolom atau fiture/class. # menentukan value atau jenis pada atribut def banyak_elemen (kolom, data): kelas=[] for i in range (len(data)): if data.values.tolist()[i][kolom] not in kelas: kelas.append(data.values.tolist()[i][kolom]) return kelas kelas=banyak_elemen(df.shape[1]-1, df) outlook=banyak_elemen(df.shape[1]-5,df) temp=banyak_elemen(df.shape[1]-4,df) humidity=banyak_elemen(df.shape[1]-3,df) windy=banyak_elemen(df.shape[1]-2,df) print(kelas,outlook,temp,humidity,windy)` ['no', 'yes'] ['sunny', 'overcast', 'rainy'] ['hot', 'mild', 'cool'] ['high', 'normal'] [False, True] Fungsi countvKelas untuk menghitung berapa perbandingan setiap elemen yang terdapat di class. # menentukan count value pada Kelas def countvKelas(kelas,kolomKelas,data): hasil=[] for x in range(len(kelas)): hasil.append(0) for i in range (len(data)): for j in range (len(kelas)): if data.values.tolist()[i][kolomKelas] == kelas[j]: hasil[j]+=1 return hasil pKelas=countvKelas(kelas,df.shape[1]-1,df) pKelas [5, 9] Fungsi entropy untuk Menghitung nilai entropy pada sebuah fiture/class. fungsi e_list untuk mempermudah penghitungan entropy setiap elemen di dalam sebuah fiture. # menentukan nilai entropy target def entropy(T): hasil=0 jumlah=0 for y in T: jumlah+=y for z in range (len(T)): if jumlah!=0: T[z]=T[z]/jumlah for i in T: if i != 0: hasil-=i*math.log(i,2) return hasil def e_list(atribut,n): temp=[] tx=t_list(atribut,n) for i in range (len(atribut)): ent=entropy(tx[i]) temp.append(ent) return temp tOutlook=t_list(outlook,5) tTemp=t_list(temp,4) tHum=t_list(humidity,3) tWin=t_list(windy,2) print(\"Sunny, Overcast, Rainy\",eOutlook) print(\"Hot, Mild, Cold\", eTemp) print(\"High, Normal\", eHum) print(\"False, True\", eWin) Sunny, Overcast, Rainy [0.9709505944546686, 0.0, 0.9709505944546686] Hot, Mild, Cold [1.0, 0.9182958340544896, 0.8112781244591328] High, Normal [0.9852281360342516, 0.5916727785823275] False, True [0.8112781244591328, 1.0] berikut contoh data yang akan di rubah menjadi decision tree \u200b 0 1 2 3 4 0 CASTEMER ID GENDER CAR TIPE SHIRT SIZE CLASS 1 1 M FAMILY SMALL C0 2 2 M SPORT MEDIUM C0 3 3 M SPORT MEDIUM C0 4 4 M SPORT LARGE C0 5 5 M SPORT EXTRA LARGE C0 6 6 M SPORT EXTRA LARGE C0 7 7 F SPORT SMALL C0 8 8 F SPORT SMALL C0 9 9 F SPORT MEDIUM C1 10 10 F LUXURY LARGE C1 11 11 M FAMILY LARGE C1 12 12 M FAMILY EXTRA LARGE C1 13 13 M FAMILY MEDIUM C1 14 14 M LUCURY EXTRA LARGE C1 15 15 F LUCURY SMALL C1 16 16 F LUCURY SMALL C1 17 17 F LUCURY MEDIUM C1 18 18 F LUCURY MEDIUM C1 19 19 F LUCURY MEDIUM C1 20 20 F LUCURY LARGE C1 pertama mencari *entropy(s)* dari kolom class di atas diket: co=10 = Pi=10/20 c1=10=Pi=10/20 $$ Entropy(S)={\\sum \\limits_{i=1}^{n} -pi\\quad log2\\quad pi} $$ $$ Entropy(S)= -10/20 * log2 10/20 -10/20 *log2 10/20 $$ $$ Entropy(S)= 1 $$ lalu kita menghitu gain setiap kolom di atas: $$ GAIN(GENDER)= entropy(S)-{\\sum \\limits_{i=1}^{n} \\frac{|Si|}{|s|}*entropy(Si)} $$ GAIN(GENDER)= 1-[10/20(6,4)+10/20(4,6)] = 1-10/20(-6/10 x log2 6/10 - 4/10 x log2 4/10) +10/20(-4/10 x log2 4/10 - 6/10 x log2 6/10 ) =1-(10/20 x 0,970951)+(10/20 x 0,970951) =1-(0,4485475+0,4485475) =1-0,970951 =0.029049 $$ GAIN(CAR\\quad TIPE)= entropy(S)-{\\sum \\limits_{i=1}^{n} \\frac{|Si|}{|s|}*entropy(Si)} $$ GAIN(CAR TIPE)= 1-[4/20(1,3)+8/20(8,0)+8/20(1,7)] = 1-4/20(-1/4 x log2 1/4 - 3/4 x log2 3/4) +8/20(-8/8 x log2 8/8 - 0/8 x log2 0/8 )+8/20(-1/8 x log2 1/8 - 7/8 x log2 7/8) =1-(0,162256+0+0,217426) =1-0,379681 =0,620319 GAIN(shirt hat)= 1-[5/20(3,2)+7/20(3,4)+4/20(2,2)+4/20(2,2)] = 1-5/20(-3/5 x log2 3/5 - 2/5 x log2 2/45 +7/20(-3/7 x log2 3/7 - 4/7 x log2 4/7 )+4/20(-2/4 x log2 2/4 - 2/2 x log2 2/2)+4/20(-2/4 log2 2/4-2/4 log2 2/4) =1-(0,242738+0,34483+0,2+0,2) =1-0,987567 =0,012433 ### Gini Index Dalam penerapan GINI index untuk data berskala continuous, terdapat beberapa metode yang dapat digunakan untuk menentukan titik pemecah terbaik, yakni metode *brute-force* dan metode *midpoints*. # Importing the required packages import numpy as np import pandas as pd from sklearn.metrics import confusion_matrix from sklearn.model_selection import train_test_split from sklearn.tree import DecisionTreeClassifier from sklearn.metrics import accuracy_score from sklearn.metrics import classification_report # Function importing Dataset def importdata (): balance_data = pd . read_csv ( 'https://archive.ics.uci.edu/ml/machine-learning-' + 'databases/balance-scale/balance-scale.data' , sep = ',' , header = None ) # Printing the dataswet shape print ( \"Dataset Length: \" , len ( balance_data )) print ( \"Dataset Shape: \" , balance_data . shape ) # Printing the dataset obseravtions print ( \"Dataset: \" , balance_data . head ()) return balance_data # Function to split the dataset def splitdataset ( balance_data ): # Seperating the target variable X = balance_data . values [:, 1 : 5 ] Y = balance_data . values [:, 0 ] # Spliting the dataset into train and test X_train , X_test , y_train , y_test = train_test_split ( X , Y , test_size = 0.3 , random_state = 100 ) return X , Y , X_train , X_test , y_train , y_test # Function to perform training with giniIndex. def train_using_gini ( X_train , X_test , y_train ): # Creating the classifier object clf_gini = DecisionTreeClassifier ( criterion = \"gini\" , random_state = 100 , max_depth = 3 , min_samples_leaf = 5 ) # Performing training clf_gini . fit ( X_train , y_train ) return clf_gini # Function to perform training with entropy. def tarin_using_entropy ( X_train , X_test , y_train ): # Decision tree with entropy clf_entropy = DecisionTreeClassifier ( criterion = \"entropy\" , random_state = 100 , max_depth = 3 , min_samples_leaf = 5 ) # Performing training clf_entropy . fit ( X_train , y_train ) return clf_entropy # Function to make predictions def prediction ( X_test , clf_object ): # Predicton on test with giniIndex y_pred = clf_object . predict ( X_test ) print ( \"Predicted values:\" ) print ( y_pred ) return y_pred # Function to calculate accuracy def cal_accuracy ( y_test , y_pred ): print ( \"Confusion Matrix: \" , confusion_matrix ( y_test , y_pred )) print ( \"Accuracy : \" , accuracy_score ( y_test , y_pred ) * 100 ) print ( \"Report : \" , classification_report ( y_test , y_pred )) # Driver code def main (): # Building Phase data = importdata () X , Y , X_train , X_test , y_train , y_test = splitdataset ( data ) clf_gini = train_using_gini ( X_train , X_test , y_train ) clf_entropy = tarin_using_entropy ( X_train , X_test , y_train ) # Operational Phase print ( \"Results Using Gini Index:\" ) # Prediction using gini y_pred_gini = prediction ( X_test , clf_gini ) cal_accuracy ( y_test , y_pred_gini ) print ( \"Results Using Entropy:\" ) # Prediction using entropy y_pred_entropy = prediction ( X_test , clf_entropy ) cal_accuracy ( y_test , y_pred_entropy ) # Calling main function if __name__ == \"__main__\" : main () Dataset Length : 625 Dataset Shape : ( 625 , 5 ) Dataset : 0 1 2 3 4 0 B 1 1 1 1 1 R 1 1 1 2 2 R 1 1 1 3 3 R 1 1 1 4 4 R 1 1 1 5 Results Using Gini Index : Predicted values : [ 'R' 'L' 'R' 'R' 'R' 'L' 'R' 'L' 'L' 'L' 'R' 'L' 'L' 'L' 'R' 'L' 'R' 'L' 'L' 'R' 'L' 'R' 'L' 'L' 'R' 'L' 'L' 'L' 'R' 'L' 'L' 'L' 'R' 'L' 'L' 'L' 'L' 'R' 'L' 'L' 'R' 'L' 'R' 'L' 'R' 'R' 'L' 'L' 'R' 'L' 'R' 'R' 'L' 'R' 'R' 'L' 'R' 'R' 'L' 'L' 'R' 'R' 'L' 'L' 'L' 'L' 'L' 'R' 'R' 'L' 'L' 'R' 'R' 'L' 'R' 'L' 'R' 'R' 'R' 'L' 'R' 'L' 'L' 'L' 'L' 'R' 'R' 'L' 'R' 'L' 'R' 'R' 'L' 'L' 'L' 'R' 'R' 'L' 'L' 'L' 'R' 'L' 'R' 'R' 'R' 'R' 'R' 'R' 'R' 'L' 'R' 'L' 'R' 'R' 'L' 'R' 'R' 'R' 'R' 'R' 'L' 'R' 'L' 'L' 'L' 'L' 'L' 'L' 'L' 'R' 'R' 'R' 'R' 'L' 'R' 'R' 'R' 'L' 'L' 'R' 'L' 'R' 'L' 'R' 'L' 'L' 'R' 'L' 'L' 'R' 'L' 'R' 'L' 'R' 'R' 'R' 'L' 'R' 'R' 'R' 'R' 'R' 'L' 'L' 'R' 'R' 'R' 'R' 'L' 'R' 'R' 'R' 'L' 'R' 'L' 'L' 'L' 'L' 'R' 'R' 'L' 'R' 'R' 'L' 'L' 'R' 'R' 'R' ] Confusion Matrix : [[ 0 6 7 ] [ 0 67 18 ] [ 0 19 71 ]] Accuracy : 73.40425531914893 Report : precision recall f1 - score support B 0.00 0.00 0.00 13 L 0.73 0.79 0.76 85 R 0.74 0.79 0.76 90 accuracy 0.73 188 macro avg 0.49 0.53 0.51 188 weighted avg 0.68 0.73 0.71 188 Results Using Entropy : Predicted values : [ 'R' 'L' 'R' 'L' 'R' 'L' 'R' 'L' 'R' 'R' 'R' 'R' 'L' 'L' 'R' 'L' 'R' 'L' 'L' 'R' 'L' 'R' 'L' 'L' 'R' 'L' 'R' 'L' 'R' 'L' 'R' 'L' 'R' 'L' 'L' 'L' 'L' 'L' 'R' 'L' 'R' 'L' 'R' 'L' 'R' 'R' 'L' 'L' 'R' 'L' 'L' 'R' 'L' 'L' 'R' 'L' 'R' 'R' 'L' 'R' 'R' 'R' 'L' 'L' 'R' 'L' 'L' 'R' 'L' 'L' 'L' 'R' 'R' 'L' 'R' 'L' 'R' 'R' 'R' 'L' 'R' 'L' 'L' 'L' 'L' 'R' 'R' 'L' 'R' 'L' 'R' 'R' 'L' 'L' 'L' 'R' 'R' 'L' 'L' 'L' 'R' 'L' 'L' 'R' 'R' 'R' 'R' 'R' 'R' 'L' 'R' 'L' 'R' 'R' 'L' 'R' 'R' 'L' 'R' 'R' 'L' 'R' 'R' 'R' 'L' 'L' 'L' 'L' 'L' 'R' 'R' 'R' 'R' 'L' 'R' 'R' 'R' 'L' 'L' 'R' 'L' 'R' 'L' 'R' 'L' 'R' 'R' 'L' 'L' 'R' 'L' 'R' 'R' 'R' 'R' 'R' 'L' 'R' 'R' 'R' 'R' 'R' 'R' 'L' 'R' 'L' 'R' 'R' 'L' 'R' 'L' 'R' 'L' 'R' 'L' 'L' 'L' 'L' 'L' 'R' 'R' 'R' 'L' 'L' 'L' 'R' 'R' 'R' ] Confusion Matrix : [[ 0 6 7 ] [ 0 63 22 ] [ 0 20 70 ]] Accuracy : 70.74468085106383 Report : precision recall f1 - score support B 0.00 0.00 0.00 13 L 0.71 0.74 0.72 85 R 0.71 0.78 0.74 90 accuracy 0.71 188 macro avg 0.47 0.51 0.49 188 weighted avg 0.66 0.71 0.68 188 MathJax.Hub.Config({ tex2jax: {inlineMath: [['$$','$$']]} }); ]","title":"decision trees"},{"location":"pohon keputusan/#decision-tree","text":"Decision tree adalah salah satu metode klasifikasi yang paling populer, karena mudah untuk diinterpretasi oleh manusia. Decision tree merupakan model prediksi menggunakan struktur pohon atau struktur berhirarki. Setiap orang tentu menginginkan sebuah pengambilan keputusan yang tepat dan efisien tak terkecuali sebuah perusahaan. Untuk itu banyak sekali perusahaan yang membutuhkan suatu media seperti Business Intellegence guna membantu dalam pengambilan keputusan yang tepat. Namun, hal tersebut tidak akan berarti tanpa adanya konsep decision tree atau yang biasa disebut pohon keputusan. Konsep dari pohon keputusan adalah mengubah data menjadi decision tree dan aturan-aturan keputusan. Manfaat utama dari penggunaan decision tree adalah kemampuannya untuk mem- break down proses pengambilan keputusan yang kompleks menjadi lebih simple, sehingga pengambil keputusan akan lebih menginterpretasikan solusi dari permasalahan. Nama lain dari decision tree adalah CART ( Classification and Regression Tree ). Dimana metode ini merupakan gabungan dari dua jenis pohon, yaitu classification tree dan juga regression tree . Untuk memudahkan, berikut ilustrasi dari keduanya. Decision tree juga berguna untuk mengeksplorasi data, menemukan hubungan tersembunyi antara sejumlah calon variabel input dengan sebuah variabel target. Decision tree memadukan antara eksplorasi data dan pemodelan, sehingga sangat bagus sebagai langkah awal dalam proses pemodelan bahkan ketika dijadikan sebagai model akhir dari beberapa teknik lain.","title":"DECISION TREE"},{"location":"pohon keputusan/#entropy","text":"Entropi adalah nilai informasi yang menyatakan ukuran ketidakpastian(impurity) dari attribut dari suatu kumpulan obyek data dalam satuan bit. $$ E(T) = \\sum_{i=1}^n {-P_i\\log{P_i}} $$ Dimana : T = ruang sampel data yang digunakan untukdata pelatihan Pi = Probabiliti muncul dalam row","title":"Entropy"},{"location":"pohon keputusan/#gain","text":"Information Gain adalah ukuran efektifitas suatu atribut dlm mengklasifikasikan data, digunakan untuk menentukan urutan atribut dimana attribut yang memiliki nilai Information Gain terbesar yang dipilih. $$ \\operatorname{Gain}(T, X) = \\operatorname{Entropy}(T) - \\sum_{v\\in{T}} \\frac{T_{X,v}}{T} E(T_{X,v}) $$ Dimana : Entropy (T) = nilai entropi total dari atribut keputusan dalam ruang sampel data T x = fitur","title":"Gain"},{"location":"pohon keputusan/#cara-membuat-decision-tree","text":"\u200b Ada beberapa cara membuat decision tree disini saya akan membuat dengan cara mengurutkan poperty yang paling penting.sebulum itu kita harus tau rumus rumusnya berikut ini rumus dari entropy dan gain : $$ Entropy(S)={\\sum \\limits_{i=1}^{n} -pi\\quad log_2\\quad pi} $$ keterangan: S=Himpunan kasus n = jumlah partisi S pi= proposi Si terhadap S kemudian hitung nilai gain menggunakan rumus : $$ GAIN(S,A)= entropy(S)-{\\sum \\limits_{i=1}^{n} \\frac{|Si|}{|s|}*entropy(Si)} $$ keterangan: S=himpunan kasus n=jumlah partisi S |si|=proporsi terhadap S |s|=jumlah kasus dalam S untuk mempermudah penghitungan saya menggunakan fungsi pembantu, seperti fungsi banyak_elemen untuk mengecek ada berapa elemen dalam sebuah kolom atau fiture/class. # menentukan value atau jenis pada atribut def banyak_elemen (kolom, data): kelas=[] for i in range (len(data)): if data.values.tolist()[i][kolom] not in kelas: kelas.append(data.values.tolist()[i][kolom]) return kelas kelas=banyak_elemen(df.shape[1]-1, df) outlook=banyak_elemen(df.shape[1]-5,df) temp=banyak_elemen(df.shape[1]-4,df) humidity=banyak_elemen(df.shape[1]-3,df) windy=banyak_elemen(df.shape[1]-2,df) print(kelas,outlook,temp,humidity,windy)` ['no', 'yes'] ['sunny', 'overcast', 'rainy'] ['hot', 'mild', 'cool'] ['high', 'normal'] [False, True] Fungsi countvKelas untuk menghitung berapa perbandingan setiap elemen yang terdapat di class. # menentukan count value pada Kelas def countvKelas(kelas,kolomKelas,data): hasil=[] for x in range(len(kelas)): hasil.append(0) for i in range (len(data)): for j in range (len(kelas)): if data.values.tolist()[i][kolomKelas] == kelas[j]: hasil[j]+=1 return hasil pKelas=countvKelas(kelas,df.shape[1]-1,df) pKelas [5, 9] Fungsi entropy untuk Menghitung nilai entropy pada sebuah fiture/class. fungsi e_list untuk mempermudah penghitungan entropy setiap elemen di dalam sebuah fiture. # menentukan nilai entropy target def entropy(T): hasil=0 jumlah=0 for y in T: jumlah+=y for z in range (len(T)): if jumlah!=0: T[z]=T[z]/jumlah for i in T: if i != 0: hasil-=i*math.log(i,2) return hasil def e_list(atribut,n): temp=[] tx=t_list(atribut,n) for i in range (len(atribut)): ent=entropy(tx[i]) temp.append(ent) return temp tOutlook=t_list(outlook,5) tTemp=t_list(temp,4) tHum=t_list(humidity,3) tWin=t_list(windy,2) print(\"Sunny, Overcast, Rainy\",eOutlook) print(\"Hot, Mild, Cold\", eTemp) print(\"High, Normal\", eHum) print(\"False, True\", eWin) Sunny, Overcast, Rainy [0.9709505944546686, 0.0, 0.9709505944546686] Hot, Mild, Cold [1.0, 0.9182958340544896, 0.8112781244591328] High, Normal [0.9852281360342516, 0.5916727785823275] False, True [0.8112781244591328, 1.0] berikut contoh data yang akan di rubah menjadi decision tree \u200b 0 1 2 3 4 0 CASTEMER ID GENDER CAR TIPE SHIRT SIZE CLASS 1 1 M FAMILY SMALL C0 2 2 M SPORT MEDIUM C0 3 3 M SPORT MEDIUM C0 4 4 M SPORT LARGE C0 5 5 M SPORT EXTRA LARGE C0 6 6 M SPORT EXTRA LARGE C0 7 7 F SPORT SMALL C0 8 8 F SPORT SMALL C0 9 9 F SPORT MEDIUM C1 10 10 F LUXURY LARGE C1 11 11 M FAMILY LARGE C1 12 12 M FAMILY EXTRA LARGE C1 13 13 M FAMILY MEDIUM C1 14 14 M LUCURY EXTRA LARGE C1 15 15 F LUCURY SMALL C1 16 16 F LUCURY SMALL C1 17 17 F LUCURY MEDIUM C1 18 18 F LUCURY MEDIUM C1 19 19 F LUCURY MEDIUM C1 20 20 F LUCURY LARGE C1 pertama mencari *entropy(s)* dari kolom class di atas diket: co=10 = Pi=10/20 c1=10=Pi=10/20 $$ Entropy(S)={\\sum \\limits_{i=1}^{n} -pi\\quad log2\\quad pi} $$ $$ Entropy(S)= -10/20 * log2 10/20 -10/20 *log2 10/20 $$ $$ Entropy(S)= 1 $$ lalu kita menghitu gain setiap kolom di atas: $$ GAIN(GENDER)= entropy(S)-{\\sum \\limits_{i=1}^{n} \\frac{|Si|}{|s|}*entropy(Si)} $$ GAIN(GENDER)= 1-[10/20(6,4)+10/20(4,6)] = 1-10/20(-6/10 x log2 6/10 - 4/10 x log2 4/10) +10/20(-4/10 x log2 4/10 - 6/10 x log2 6/10 ) =1-(10/20 x 0,970951)+(10/20 x 0,970951) =1-(0,4485475+0,4485475) =1-0,970951 =0.029049 $$ GAIN(CAR\\quad TIPE)= entropy(S)-{\\sum \\limits_{i=1}^{n} \\frac{|Si|}{|s|}*entropy(Si)} $$ GAIN(CAR TIPE)= 1-[4/20(1,3)+8/20(8,0)+8/20(1,7)] = 1-4/20(-1/4 x log2 1/4 - 3/4 x log2 3/4) +8/20(-8/8 x log2 8/8 - 0/8 x log2 0/8 )+8/20(-1/8 x log2 1/8 - 7/8 x log2 7/8) =1-(0,162256+0+0,217426) =1-0,379681 =0,620319 GAIN(shirt hat)= 1-[5/20(3,2)+7/20(3,4)+4/20(2,2)+4/20(2,2)] = 1-5/20(-3/5 x log2 3/5 - 2/5 x log2 2/45 +7/20(-3/7 x log2 3/7 - 4/7 x log2 4/7 )+4/20(-2/4 x log2 2/4 - 2/2 x log2 2/2)+4/20(-2/4 log2 2/4-2/4 log2 2/4) =1-(0,242738+0,34483+0,2+0,2) =1-0,987567 =0,012433 ### Gini Index Dalam penerapan GINI index untuk data berskala continuous, terdapat beberapa metode yang dapat digunakan untuk menentukan titik pemecah terbaik, yakni metode *brute-force* dan metode *midpoints*. # Importing the required packages import numpy as np import pandas as pd from sklearn.metrics import confusion_matrix from sklearn.model_selection import train_test_split from sklearn.tree import DecisionTreeClassifier from sklearn.metrics import accuracy_score from sklearn.metrics import classification_report # Function importing Dataset def importdata (): balance_data = pd . read_csv ( 'https://archive.ics.uci.edu/ml/machine-learning-' + 'databases/balance-scale/balance-scale.data' , sep = ',' , header = None ) # Printing the dataswet shape print ( \"Dataset Length: \" , len ( balance_data )) print ( \"Dataset Shape: \" , balance_data . shape ) # Printing the dataset obseravtions print ( \"Dataset: \" , balance_data . head ()) return balance_data # Function to split the dataset def splitdataset ( balance_data ): # Seperating the target variable X = balance_data . values [:, 1 : 5 ] Y = balance_data . values [:, 0 ] # Spliting the dataset into train and test X_train , X_test , y_train , y_test = train_test_split ( X , Y , test_size = 0.3 , random_state = 100 ) return X , Y , X_train , X_test , y_train , y_test # Function to perform training with giniIndex. def train_using_gini ( X_train , X_test , y_train ): # Creating the classifier object clf_gini = DecisionTreeClassifier ( criterion = \"gini\" , random_state = 100 , max_depth = 3 , min_samples_leaf = 5 ) # Performing training clf_gini . fit ( X_train , y_train ) return clf_gini # Function to perform training with entropy. def tarin_using_entropy ( X_train , X_test , y_train ): # Decision tree with entropy clf_entropy = DecisionTreeClassifier ( criterion = \"entropy\" , random_state = 100 , max_depth = 3 , min_samples_leaf = 5 ) # Performing training clf_entropy . fit ( X_train , y_train ) return clf_entropy # Function to make predictions def prediction ( X_test , clf_object ): # Predicton on test with giniIndex y_pred = clf_object . predict ( X_test ) print ( \"Predicted values:\" ) print ( y_pred ) return y_pred # Function to calculate accuracy def cal_accuracy ( y_test , y_pred ): print ( \"Confusion Matrix: \" , confusion_matrix ( y_test , y_pred )) print ( \"Accuracy : \" , accuracy_score ( y_test , y_pred ) * 100 ) print ( \"Report : \" , classification_report ( y_test , y_pred )) # Driver code def main (): # Building Phase data = importdata () X , Y , X_train , X_test , y_train , y_test = splitdataset ( data ) clf_gini = train_using_gini ( X_train , X_test , y_train ) clf_entropy = tarin_using_entropy ( X_train , X_test , y_train ) # Operational Phase print ( \"Results Using Gini Index:\" ) # Prediction using gini y_pred_gini = prediction ( X_test , clf_gini ) cal_accuracy ( y_test , y_pred_gini ) print ( \"Results Using Entropy:\" ) # Prediction using entropy y_pred_entropy = prediction ( X_test , clf_entropy ) cal_accuracy ( y_test , y_pred_entropy ) # Calling main function if __name__ == \"__main__\" : main () Dataset Length : 625 Dataset Shape : ( 625 , 5 ) Dataset : 0 1 2 3 4 0 B 1 1 1 1 1 R 1 1 1 2 2 R 1 1 1 3 3 R 1 1 1 4 4 R 1 1 1 5 Results Using Gini Index : Predicted values : [ 'R' 'L' 'R' 'R' 'R' 'L' 'R' 'L' 'L' 'L' 'R' 'L' 'L' 'L' 'R' 'L' 'R' 'L' 'L' 'R' 'L' 'R' 'L' 'L' 'R' 'L' 'L' 'L' 'R' 'L' 'L' 'L' 'R' 'L' 'L' 'L' 'L' 'R' 'L' 'L' 'R' 'L' 'R' 'L' 'R' 'R' 'L' 'L' 'R' 'L' 'R' 'R' 'L' 'R' 'R' 'L' 'R' 'R' 'L' 'L' 'R' 'R' 'L' 'L' 'L' 'L' 'L' 'R' 'R' 'L' 'L' 'R' 'R' 'L' 'R' 'L' 'R' 'R' 'R' 'L' 'R' 'L' 'L' 'L' 'L' 'R' 'R' 'L' 'R' 'L' 'R' 'R' 'L' 'L' 'L' 'R' 'R' 'L' 'L' 'L' 'R' 'L' 'R' 'R' 'R' 'R' 'R' 'R' 'R' 'L' 'R' 'L' 'R' 'R' 'L' 'R' 'R' 'R' 'R' 'R' 'L' 'R' 'L' 'L' 'L' 'L' 'L' 'L' 'L' 'R' 'R' 'R' 'R' 'L' 'R' 'R' 'R' 'L' 'L' 'R' 'L' 'R' 'L' 'R' 'L' 'L' 'R' 'L' 'L' 'R' 'L' 'R' 'L' 'R' 'R' 'R' 'L' 'R' 'R' 'R' 'R' 'R' 'L' 'L' 'R' 'R' 'R' 'R' 'L' 'R' 'R' 'R' 'L' 'R' 'L' 'L' 'L' 'L' 'R' 'R' 'L' 'R' 'R' 'L' 'L' 'R' 'R' 'R' ] Confusion Matrix : [[ 0 6 7 ] [ 0 67 18 ] [ 0 19 71 ]] Accuracy : 73.40425531914893 Report : precision recall f1 - score support B 0.00 0.00 0.00 13 L 0.73 0.79 0.76 85 R 0.74 0.79 0.76 90 accuracy 0.73 188 macro avg 0.49 0.53 0.51 188 weighted avg 0.68 0.73 0.71 188 Results Using Entropy : Predicted values : [ 'R' 'L' 'R' 'L' 'R' 'L' 'R' 'L' 'R' 'R' 'R' 'R' 'L' 'L' 'R' 'L' 'R' 'L' 'L' 'R' 'L' 'R' 'L' 'L' 'R' 'L' 'R' 'L' 'R' 'L' 'R' 'L' 'R' 'L' 'L' 'L' 'L' 'L' 'R' 'L' 'R' 'L' 'R' 'L' 'R' 'R' 'L' 'L' 'R' 'L' 'L' 'R' 'L' 'L' 'R' 'L' 'R' 'R' 'L' 'R' 'R' 'R' 'L' 'L' 'R' 'L' 'L' 'R' 'L' 'L' 'L' 'R' 'R' 'L' 'R' 'L' 'R' 'R' 'R' 'L' 'R' 'L' 'L' 'L' 'L' 'R' 'R' 'L' 'R' 'L' 'R' 'R' 'L' 'L' 'L' 'R' 'R' 'L' 'L' 'L' 'R' 'L' 'L' 'R' 'R' 'R' 'R' 'R' 'R' 'L' 'R' 'L' 'R' 'R' 'L' 'R' 'R' 'L' 'R' 'R' 'L' 'R' 'R' 'R' 'L' 'L' 'L' 'L' 'L' 'R' 'R' 'R' 'R' 'L' 'R' 'R' 'R' 'L' 'L' 'R' 'L' 'R' 'L' 'R' 'L' 'R' 'R' 'L' 'L' 'R' 'L' 'R' 'R' 'R' 'R' 'R' 'L' 'R' 'R' 'R' 'R' 'R' 'R' 'L' 'R' 'L' 'R' 'R' 'L' 'R' 'L' 'R' 'L' 'R' 'L' 'L' 'L' 'L' 'L' 'R' 'R' 'R' 'L' 'L' 'L' 'R' 'R' 'R' ] Confusion Matrix : [[ 0 6 7 ] [ 0 63 22 ] [ 0 20 70 ]] Accuracy : 70.74468085106383 Report : precision recall f1 - score support B 0.00 0.00 0.00 13 L 0.71 0.74 0.72 85 R 0.71 0.78 0.74 90 accuracy 0.71 188 macro avg 0.47 0.51 0.49 188 weighted avg 0.66 0.71 0.68 188 MathJax.Hub.Config({ tex2jax: {inlineMath: [['$$','$$']]} }); ]","title":"CARA MEMBUAT DECISION TREE"}]}